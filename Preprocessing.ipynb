{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_num_punct (text):\n",
    "    text = re.sub(r'([^a-zA-Z ]+?)', ' ', text)\n",
    "    text = text.replace('X', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def transform_text(series, save_as):\n",
    "    vectorizer = TfidfVectorizer(norm='l2',lowercase=True, use_idf=True, sublinear_tf=True).fit(series)\n",
    "    pickle.dump(vectorizer, open(save_as, 'wb'))\n",
    "    vec_train = vectorizer.transform(series)\n",
    "    return vec_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'dataset_product.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, 'rb') as handle:\n",
    "    dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = dataset['X_train'].copy().reset_index(drop=True)\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow some text along the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Remove numbers, punctuation, capitalized Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)\n",
    "X_train = X_train.apply(clean_num_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Choose a language and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since texts also contain proper nouns (wells fargo, citibank) that might be helpful for classification but might not be found in a language's dictionary, we will not do anything here. It also seems that the texts are made up of American English considering the location (state/zip) provided with each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "X_train = X_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) and len(word)>1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning\n",
    "#X_train_before = dataset['X_train'].reset_index(drop=True)\n",
    "#X_train_before[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning\n",
    "#X_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming, Lemmatization, N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: time this\n",
    "train_matrix = transform_text(X_train, 'tfidf_vec_product.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((109963, 57940), (109963,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([45839, 45584, 45371, 33333, 31565, 26351, 24627, 23696, 22352,\n",
       "       20655, 20603, 16293, 12522,  9531,  8352,  7943,  7544,  7361,\n",
       "        3538], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our example from above, indexes that are filled (corresponds to set of words in text)\n",
    "print(len(train_matrix[5].nonzero()[1]) == len(set(X_train[5].split(' '))))\n",
    "train_matrix[5].nonzero()[1] # Indices in sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index: 72576\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Are all of the unique cleaned words captured in this huge, sparse matrix?\n",
    "for i in range(X_train.shape[0]):\n",
    "    if len(train_matrix[i].nonzero()[1])==len(set(X_train[i].split(\" \"))):\n",
    "        pass\n",
    "    else:\n",
    "        print('Error at index: ' + str(i))\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite yet, but it's a lot better than it was. I was able to find out about this data's peculiarities by investigating the above indices. For example: new lines and single character words (often typos or remainders of contractions). It might not be ideal to sort these out, but for now it helps massively with dimension reduction. Ultimately, it's a trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty samples from  training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109963,) (109963, 57940)\n",
      "(109962,) (109962, 57940)\n"
     ]
    }
   ],
   "source": [
    "#print(X_train[72576]) # Before\n",
    "print(X_train.shape, train_matrix.shape) \n",
    "X_train = X_train.drop(index=72576).reset_index(drop=True)\n",
    "train_matrix = transform_text(X_train, 'tfidf_vec_product.pkl')\n",
    "#print(X_train[72576]) # After\n",
    "print(X_train.shape, train_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Still need to remove element from y_train. Handled before training. Needs a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "for i in range((train_matrix.shape[0])):\n",
    "    if train_matrix[i].nnz == train_matrix.shape[1]:\n",
    "        print('Error at: ' + str(i))\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sparse matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name= 'X_train_product.npz'\n",
    "#sparse.save_npz(file_name, train_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for validation/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "tf = pickle.load(open('tfidf_vec_product.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = dataset['X_val'].copy()\n",
    "series = series.reset_index(drop=True)\n",
    "series = series.apply(clean_num_punct)\n",
    "series = series.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) and len(word)>1]))\n",
    "# Transform to vector\n",
    "val_matrix = tf.transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "# Check to see if any are all zero\n",
    "for i in range((val_matrix.shape[0])):\n",
    "    if val_matrix[i].nnz == val_matrix.shape[1]:\n",
    "        print('Error at: ' + str(i))\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_text(text_as_series, tf_file, stopwords):\n",
    "    tf = pickle.load(open(tf_file, 'rb'))\n",
    "    series = text_as_series.reset_index(drop=True)\n",
    "\n",
    "    def clean_num_punct(text):\n",
    "        text = re.sub(r'([^a-zA-Z ]+?)', ' ', text)\n",
    "        text = text.replace('X', '')\n",
    "        text = text.replace('\\n', ' ')\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        word_list = text.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    series = series.apply(clean_num_punct)\n",
    "    series = series.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) and len(word) > 1]))\n",
    "    matrix = tf.transform(series)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest_mtx = clean_raw_text(dataset['X_val'].copy(), 'tfidf_vec_product.pkl', stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17874, 57940) (17874, 57940)\n",
      "(17874, 57940) (17874,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(mytest_mtx.shape, val_matrix.shape)\n",
    "print(val_matrix.shape, dataset['y_val'].shape)\n",
    "print((mytest_mtx!=val_matrix).nnz==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for training validation (Test data will be processed in predict.py)\n",
    "#file_name= 'X_val_product.npz'\n",
    "#sparse.save_npz(file_name, val_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
